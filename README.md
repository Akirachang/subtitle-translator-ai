<div align="center">

# Subtitle Translator AI

<p align="center">
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.9+-blue.svg" alt="Python 3.9+"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.8+-ee4c2c.svg?logo=pytorch&logoColor=white" alt="PyTorch"></a>
  <a href="https://huggingface.co/docs/transformers/"><img src="https://img.shields.io/badge/Transformers-4.55+-yellow.svg?logo=huggingface&logoColor=white" alt="Transformers"></a>
  <a href="https://docs.pydantic.dev/"><img src="https://img.shields.io/badge/Pydantic-2.11+-e92063.svg?logo=pydantic&logoColor=white" alt="Pydantic"></a>
  <a href="https://huggingface.co/facebook/nllb-200-distilled-600M"><img src="https://img.shields.io/badge/%F0%9F%A4%97-NLLB--200-FFD21E.svg" alt="NLLB-200"></a>
</p>
</div>

Translate .srt subtitle files using Hugging Face Transformers (NLLB-200). Provides a simple Python API and a CLI.

## ðŸ“– Overview

This tool translates SRT subtitle files while preserving timing, formatting, and structure. It uses specialized translation models that run entirely on your local machine, providing consistent and reliable outputs without the unpredictability of general-purpose LLMs.

### Motivation

I use DaVinci Resolve to automatically generate subtitles. It does a great job with timing and segmentation, but it doesn't translate between languages. I tried ChatGPT and other LLMs, but long videos create extremely long contexts; chunking them naÃ¯vely often leads to hallucinations, mixed languages, or lost formatting.

This tool uses smaller, specialized translation models ([facebook/nllb-200-distilled-600M](https://huggingface.co/facebook/nllb-200-distilled-600M)) that are purpose-built for translation tasks, resulting in more consistent and reliable outputs for long-form content.

## âœ¨ Features

This project keeps the SRT file as the source of truth:

- **Preserves Structure**: Parses SRT blocks with Pydantic and maintains index/timing integrity
- **HTML Tag Support**: Keeps basic HTML tags that are comprehensible by software (e.g., DaVinci Resolve)
- **200+ Languages**: Uses Hugging Face NLP model for many language pairs (fra_Latn â†’ eng_Latn by default)
- **Dual Interface**: Both CLI and Python API for flexible integration
- **Local Processing**: No API keys required, runs entirely offline
- **Batch Processing**: Efficient translation with progress tracking using tqdm

The result is a reliable, repeatable translation pipeline that works well on long files without drifting context or breaking your editor workflow.

## ðŸš€ Installation

### Requirements

- Python 3.9 or higher
- ~2.5GB disk space for the model (downloaded on first run)

### Editable Install (Development)

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -e .
```

### Install from PyPI (Coming Soon)

```bash
pip install subtitle-translator-ai
```

**Note**: The first run will download the NLLB-200 model; expect a large download (~2.5GB) and some warm-up time.

## ðŸ’» Usage

### CLI

Basic usage:

```bash
subtitle-translator sample.srt -o outputs/translated.srt --src-lang fra_Latn --tgt-lang eng_Latn
```

**Options**:

- `--src-lang`: Source language code (default: `fra_Latn`)
- `--tgt-lang`: Target language code (default: `eng_Latn`)
- `-o, --output`: Output file path

### Python API

```python
from subtitle_translator_ai import translate_srt, SubtitleBlock

# Translate a file
translate_srt(
   "sample.srt",
   "outputs/translated.srt",
   src_lang="fra_Latn",
   tgt_lang="eng_Latn",
)

# Parse and serialize a single block
block_text = """1
00:00:01,000 --> 00:00:03,500
<b>Bonjour! Bienvenue Ã¡ Paris</b>
"""
sb = SubtitleBlock(block=block_text)
print(sb.index, sb.start_time, sb.end_time, sb.text)
print(sb.serialize())  # back to SRT block (with <b> tags preserved)
```

## âš™ï¸ Configuration

### Language Codes

The project uses NLLB-200 language codes. Common examples:

| Language              | Code       |
| --------------------- | ---------- |
| English               | `eng_Latn` |
| Spanish               | `spa_Latn` |
| French                | `fra_Latn` |
| German                | `deu_Latn` |
| Japanese              | `jpn_Jpan` |
| Korean                | `kor_Hang` |
| Chinese (Simplified)  | `zho_Hans` |
| Chinese (Traditional) | `zho_Hant` |
| Arabic                | `arb_Arab` |
| Russian               | `rus_Cyrl` |

For a complete list of supported languages, see the [NLLB-200 documentation](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200).

### Model Configuration

- **Default Model**: `facebook/nllb-200-distilled-600M`
- **Model Size**: ~2.5GB
- **Cache Location**: `~/.cache/huggingface/`
- **Batch Size**: Configurable in code (default handles long files efficiently)

## ðŸ“ Project Structure

```
subtitle-translator-ai/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ subtitle_translator_ai/
â”‚       â”œâ”€â”€ __init__.py          # Package exports
â”‚       â”œâ”€â”€ api.py               # Python API interface
â”‚       â”œâ”€â”€ cli.py               # CLI entry point
â”‚       â”œâ”€â”€ srt_translator.py   # Core translation logic
â”‚       â””â”€â”€ utils.py             # Utility functions
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ logo.png                 # Project logo
â”œâ”€â”€ outputs/                     # Translated output files
â”œâ”€â”€ sample.srt                   # Example subtitle file
â”œâ”€â”€ run.py                       # Quick start script
â”œâ”€â”€ pyproject.toml              # Project configuration
â””â”€â”€ README.md                    # This file
```

## ðŸ—ï¸ Architecture

The translation pipeline follows these steps:

1. **Parse SRT File**: Read and parse the input SRT file into structured blocks using Pydantic models
2. **Extract Text**: Extract subtitle text while preserving HTML tags and timing metadata
3. **Batch Translation**: Send text to the NLLB-200 model in efficient batches with progress tracking
4. **Reconstruct Blocks**: Combine translated text with original timing and index data
5. **Write Output**: Serialize blocks back to valid SRT format

### Key Components

- **SubtitleBlock (Pydantic Model)**: Validates and structures each subtitle entry with index, timing, and text
- **SRT Translator**: Manages model loading, batch processing, and translation logic
- **HTML Tag Preservation**: Maintains formatting tags throughout the translation process
- **Progress Tracking**: Uses tqdm to display real-time translation progress

The system uses PyTorch and Transformers under the hood, with automatic GPU acceleration if available.

## ðŸ”§ Troubleshooting

### Model Download Issues

**Problem**: Model download fails or times out

**Solution**:

```bash
# Manually download the model
python -c "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM; AutoTokenizer.from_pretrained('facebook/nllb-200-distilled-600M'); AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')"
```

### Memory Issues

**Problem**: Out of memory errors during translation

**Solution**:

- Reduce batch size in the code
- Use CPU instead of GPU for very large files
- Process files in smaller chunks

### Invalid SRT Format

**Problem**: Output SRT file has formatting issues

**Solution**:

- Ensure input SRT file is properly formatted
- Check for unusual characters in the source file
- Validate timing format (should be `HH:MM:SS,mmm --> HH:MM:SS,mmm`)

### Language Code Errors

**Problem**: `Language code not supported` error

**Solution**:

- Verify you're using NLLB-200 codes (e.g., `eng_Latn`, not `en`)
- Check the [complete language list](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)

### Performance Issues

**Problem**: Translation is very slow

**Solution**:

- First run downloads the model - this is normal
- Subsequent runs should be faster
- GPU acceleration will improve speed significantly
- Consider using a smaller subset for testing

---

## License

This project is open source. See LICENSE file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Support

If you encounter any issues or have questions, please [open an issue](https://github.com/Akirachang/subtitle-translator-ai/issues) on GitHub.
